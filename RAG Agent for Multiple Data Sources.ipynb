{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4705c5af-bc06-479f-be6e-7c96fa26f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun, BraveSearch\n",
    "from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "from langchain_community.document_loaders import WebBaseLoader, BraveSearchLoader, WikipediaLoader, PyPDFLoader, Docx2txtLoader, TextLoader, CSVLoader, BSHTMLLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "43102735-7df1-44eb-af11-8faf9a0051fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de255271-4eda-4631-b7a7-eeecd37f631f",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2a2205f3-5aee-4983-8ebf-f94871a9af18",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')\n",
    "BRAVE_API_KEY = os.getenv(\"BRAVE_SEARCH_API_KEY\")  # required for web search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f03584-7720-4849-90f5-a25ee36a9543",
   "metadata": {},
   "source": [
    "## Wikipedia (Quick search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ae98adff-daa6-4ffa-9d7c-fdf1967ec8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=1000)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bd62eaa6-9b57-4822-a241-ba0fd88f7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.description = (\n",
    "    \"Wikipedia quick lookup. Use for short factual questions that need a brief summary. \"\n",
    "    \"Do NOT use for detailed/ambiguous multi-part queries or when quotes/timelines are needed; \"\n",
    "    \"prefer wikipedia_rag in those cases.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ece9d3-735d-4fcd-a4eb-bc8c8b328a19",
   "metadata": {},
   "source": [
    "## LangSmith site Retriever tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ed499ae3-9839-40c7-907e-dd1fd16bb177",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=WebBaseLoader('https://docs.smith.langchain.com/')\n",
    "docs=loader.load()\n",
    "documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "52fa48df-5615-468c-a864-645b4b091bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x11855c440>, search_kwargs={})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## creating vector DB\n",
    "vectordb=FAISS.from_documents(documents, OpenAIEmbeddings())\n",
    "retriever=vectordb.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4402854b-e3e7-4559-9876-d3e14d633add",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"FOR LANGSMITH ONLY. If the query mentions LangSmith or docs.smith.langchain.com, \"\n",
    "    \"you MUST use this tool and no other.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "80cb2e44-c677-401b-97fd-1f793b94297b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith_search'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7cede7-c08e-49cf-945d-264efdb2184d",
   "metadata": {},
   "source": [
    "## Arxiv Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0bd218f2-159e-408c-8d1b-db561ab516c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_wrapper=ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv=ArxivQueryRun(arxiv_wrapper=arxiv_wrapper)\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8de714ee-1000-49f7-9161-3b973c5db8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv.description = (\n",
    "    \"ArXiv tool. Use when the query references arXiv or includes an arXiv ID.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ad4a4-a928-47b8-8171-bec6c26677ac",
   "metadata": {},
   "source": [
    "## Wikipedia RAG (deeper context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ec804f0c-a4d9-4e9a-964e-6e441c836de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_rag(query: str, top_docs: int = 3) -> str:\n",
    "    try:\n",
    "        wiki_docs = WikipediaLoader(query=query, load_max_docs=top_docs).load()\n",
    "    except Exception as e:\n",
    "        return f\"Failed to load from Wikipedia: {e}\"\n",
    "\n",
    "    if not wiki_docs:\n",
    "        return \"No Wikipedia pages found.\"\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = splitter.split_documents(wiki_docs)\n",
    "\n",
    "    vdb = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "    rel_docs = vdb.as_retriever(search_kwargs={\"k\": 4}).get_relevant_documents(query)\n",
    "\n",
    "    if not rel_docs:\n",
    "        return \"No relevant passages were found in the fetched Wikipedia pages.\"\n",
    "\n",
    "    out, seen = [], []\n",
    "    for d in rel_docs:\n",
    "        title = d.metadata.get(\"title\") or \"Wikipedia\"\n",
    "        if title not in seen:\n",
    "            seen.append(title)\n",
    "        text = d.page_content.strip().replace(\"\\n\", \" \")\n",
    "        if len(text) > 600:\n",
    "            text = text[:600] + \" ...\"\n",
    "        url = d.metadata.get(\"source\") or d.metadata.get(\"wikipedia_url\") or \"\"\n",
    "        src = f\"{title} — {url}\" if url else title\n",
    "        out.append(f\"- Excerpt: {text}\\n  Source: {src}\")\n",
    "\n",
    "    return (\n",
    "        \"Wikipedia RAG answer (supporting snippets):\\n\"\n",
    "        + \"\\n\".join(out[:4])\n",
    "        + \"\\n\\nPages:\\n\"\n",
    "        + \"\\n\".join(f\"* {t}\" for t in seen[:5])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0921ed22-79b3-493a-a44f-718e9612cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_rag_tool = StructuredTool.from_function(\n",
    "    func=wikipedia_rag,\n",
    "    name=\"wikipedia_rag\",\n",
    "    description=(\n",
    "        \"Deep Wikipedia RAG. Use when the question needs detailed context, comparisons, timelines, or quotations \"\n",
    "        \"from Wikipedia beyond a short summary. Args: query:str, top_docs:int=3\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f36a7-e315-46af-84b1-2d3d3d56cd91",
   "metadata": {},
   "source": [
    "## Web RAG via Brave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4d287e73-6cf2-4643-b813-51bbf175b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "brave_search_tool = BraveSearch.from_api_key(api_key=BRAVE_API_KEY, search_kwargs={\"count\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "eaa9b69d-9f10-4baf-911e-4abc7f8edb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_rag_search(query: str, count: int = 5) -> str:\n",
    "    if not BRAVE_API_KEY:\n",
    "        return \"BRAVE_SEARCH_API_KEY is not set.\"\n",
    "    count = max(1, min(int(count), 10))\n",
    "\n",
    "    # 1) Search\n",
    "    serp_docs = BraveSearchLoader(query=query, api_key=BRAVE_API_KEY, search_kwargs={\"count\": count}).load()\n",
    "\n",
    "    # 2) Links\n",
    "    links = []\n",
    "    for d in serp_docs:\n",
    "        link = d.metadata.get(\"link\")\n",
    "        if link and link not in links:\n",
    "            links.append(link)\n",
    "    if not links:\n",
    "        return \"No web results found.\"\n",
    "\n",
    "    # 3) Scrape\n",
    "    web_docs = []\n",
    "    for url in links:\n",
    "        try:\n",
    "            for pd in WebBaseLoader(url).load():\n",
    "                pd.metadata[\"source\"] = url\n",
    "                web_docs.append(pd)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not web_docs:\n",
    "        return \"Failed to load content from the top web results.\"\n",
    "\n",
    "    # 4) Chunk → 5) Embed → 6) Similarity search\n",
    "    chunks = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=200).split_documents(web_docs)\n",
    "    vdb = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "    rel_docs = vdb.as_retriever(search_kwargs={\"k\": 4}).get_relevant_documents(query)\n",
    "\n",
    "    if not rel_docs:\n",
    "        return \"No relevant passages were found in the crawled pages.\"\n",
    "\n",
    "    # 7) Compose short answer with sources\n",
    "    out, seen = [], []\n",
    "    for d in rel_docs:\n",
    "        src = d.metadata.get(\"source\") or d.metadata.get(\"link\") or \"unknown\"\n",
    "        if src not in seen:\n",
    "            seen.append(src)\n",
    "        text = d.page_content.strip().replace(\"\\n\", \" \")\n",
    "        if len(text) > 600:\n",
    "            text = text[:600] + \" ...\"\n",
    "        out.append(f\"- Excerpt: {text}\\n  Source: {src}\")\n",
    "\n",
    "    return (\n",
    "        \"Web RAG answer (supporting snippets):\\n\"\n",
    "        + \"\\n\".join(out[:4])\n",
    "        + \"\\n\\nTop sources:\\n\"\n",
    "        + \"\\n\".join(f\"* {s}\" for s in seen[:5])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "52f98337-bfbe-4c39-ae6e-940ada971202",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_rag_tool = StructuredTool.from_function(\n",
    "    func=web_rag_search,\n",
    "    name=\"web_rag_search\",\n",
    "    description=\"General web RAG. Use when the question is NOT about Wikipedia, Arxiv, or LangSmith. Args: query:str, count:int=5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5749f7e1-c916-4d53-8969-57f463c93df4",
   "metadata": {},
   "source": [
    "## Document RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3b0d5e00-0df4-47fe-8cb0-ef21f47071c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_VDB = None\n",
    "\n",
    "def doc_index(files_csv: str = \"./NehaGaonkarDataScientist.pdf\", urls_csv: str = \"\", chunk_size: int = 1200, chunk_overlap: int = 200) -> str:\n",
    "    \"\"\"\n",
    "    Ingest user-provided documents into an in-memory FAISS index.\n",
    "    - files_csv: comma-separated local file paths (pdf, docx, txt, md, csv, html)\n",
    "    - urls_csv: comma-separated URLs to fetch and index\n",
    "    \"\"\"\n",
    "    paths = [p.strip() for p in files_csv.split(\",\") if p.strip()]\n",
    "    urls  = [u.strip() for u in urls_csv.split(\",\")  if u.strip()]\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    # Load local files\n",
    "    for p in paths:\n",
    "        try:\n",
    "            lo = None\n",
    "            pl = p.lower()\n",
    "            if pl.endswith(\".pdf\"):\n",
    "                lo = PyPDFLoader(p)\n",
    "            elif pl.endswith(\".docx\") or pl.endswith(\".doc\"):\n",
    "                lo = Docx2txtLoader(p)\n",
    "            elif pl.endswith(\".csv\"):\n",
    "                lo = CSVLoader(p)\n",
    "            elif pl.endswith(\".html\") or pl.endswith(\".htm\"):\n",
    "                lo = BSHTMLLoader(p)\n",
    "            elif pl.endswith(\".txt\") or pl.endswith(\".md\"):\n",
    "                lo = TextLoader(p, encoding=\"utf-8\")\n",
    "            else:\n",
    "                # skip unknown types silently to avoid heavy 'unstructured' dependency\n",
    "                continue\n",
    "            docs.extend(lo.load())\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Load URLs\n",
    "    for url in urls:\n",
    "        try:\n",
    "            page_docs = WebBaseLoader(url).load()\n",
    "            for pd in page_docs:\n",
    "                pd.metadata[\"source\"] = url\n",
    "            docs.extend(page_docs)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if not docs:\n",
    "        return \"No documents loaded. Provide valid paths in files_csv or URLs in urls_csv.\"\n",
    "\n",
    "    # Chunk and (create or update) vectorstore\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=int(chunk_size), chunk_overlap=int(chunk_overlap))\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    global DOC_VDB, DOC_CHUNK_COUNT, DOC_INDEXED\n",
    "    if DOC_VDB is None:\n",
    "        DOC_VDB = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "    else:\n",
    "        DOC_VDB.add_documents(chunks)\n",
    "\n",
    "    DOC_CHUNK_COUNT += len(chunks)\n",
    "    DOC_INDEXED = DOC_VDB is not None and DOC_CHUNK_COUNT > 0\n",
    "\n",
    "    return f\"Indexed {len(chunks)} chunks from {len(paths)} file(s) and {len(urls)} URL(s).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c3b49ad6-5ae8-4029-8a68-102f9d770946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_rag_search(query: str, k: int = 4) -> str:\n",
    "    \"\"\"\n",
    "    Query the user-provided document corpus previously indexed with doc_index.\n",
    "    \"\"\"\n",
    "    if DOC_VDB is None:\n",
    "        return \"No indexed documents. Run doc_index first with files_csv and/or urls_csv.\"\n",
    "    rel = DOC_VDB.as_retriever(search_kwargs={\"k\": int(k)}).get_relevant_documents(query)\n",
    "    if not rel:\n",
    "        return \"No relevant passages found in the indexed documents.\"\n",
    "\n",
    "    out, seen = [], []\n",
    "    for d in rel:\n",
    "        src = d.metadata.get(\"source\") or d.metadata.get(\"file_path\") or d.metadata.get(\"source_path\") or \"uploaded_document\"\n",
    "        if src not in seen: seen.append(src)\n",
    "        text = d.page_content.strip().replace(\"\\n\", \" \")\n",
    "        if len(text) > 700: text = text[:700] + \" ...\"\n",
    "        out.append(f\"- Excerpt: {text}\\n  Source: {src}\")\n",
    "    return \"Document RAG answer (supporting snippets):\\n\" + \"\\n\".join(out[:4]) + \"\\n\\nSources:\\n\" + \"\\n\".join(f\"* {s}\" for s in seen[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "be75a313-f40e-4b30-8d70-84af14f36538",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_index_tool = StructuredTool.from_function(\n",
    "    func=doc_index,\n",
    "    name=\"doc_index\",\n",
    "    description=(\"Ingest user documents into a persistent (session) vector index. \"\n",
    "                 \"Use when the user uploads or provides links. \"\n",
    "                 \"Args: files_csv:str (comma-separated file paths), urls_csv:str (comma-separated URLs), \"\n",
    "                 \"chunk_size:int=1200, chunk_overlap:int=200\"),\n",
    ")\n",
    "doc_rag_tool = StructuredTool.from_function(\n",
    "    func=doc_rag_search,\n",
    "    name=\"doc_rag_search\",\n",
    "    description=(\"Query the user's indexed documents (from doc_index). \"\n",
    "                 \"Use for questions that refer to the uploaded/added documents. \"\n",
    "                 \"Args: query:str, k:int=4\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a10dedf-f1b7-4e63-b914-bf8edf3bffaf",
   "metadata": {},
   "source": [
    "## Combine All Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "48528f3f-e18a-4a9d-9d66-7c1998e2f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Assemble tools ---\n",
    "tools = [wiki, wikipedia_rag_tool, arxiv, retriever_tool, web_rag_tool, doc_index_tool, doc_rag_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a1dc9b08-17b4-439a-bed9-abefe2eb1bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(description='Wikipedia quick lookup. Use for short factual questions that need a brief summary. Do NOT use for detailed/ambiguous multi-part queries or when quotes/timelines are needed; prefer wikipedia_rag in those cases.', api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/Users/nehagaonkar/Desktop/RAG agent/ragagent/lib/python3.12/site-packages/wikipedia/__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=1000)),\n",
       " StructuredTool(name='wikipedia_rag', description='Deep Wikipedia RAG. Use when the question needs detailed context, comparisons, timelines, or quotations from Wikipedia beyond a short summary. Args: query:str, top_docs:int=3', args_schema=<class 'langchain_core.utils.pydantic.wikipedia_rag'>, func=<function wikipedia_rag at 0x118260900>),\n",
       " ArxivQueryRun(description='ArXiv tool. Use when the query references arXiv or includes an arXiv ID.', api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=3, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=4000)),\n",
       " Tool(name='langsmith_search', description='FOR LANGSMITH ONLY. If the query mentions LangSmith or docs.smith.langchain.com, you MUST use this tool and no other.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x108c7bd80>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x11855c440>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x1087dd4e0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x11855c440>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content')),\n",
       " StructuredTool(name='web_rag_search', description='General web RAG. Use when the question is NOT about Wikipedia, Arxiv, or LangSmith. Args: query:str, count:int=5', args_schema=<class 'langchain_core.utils.pydantic.web_rag_search'>, func=<function web_rag_search at 0x10969a200>),\n",
       " StructuredTool(name='doc_index', description='Ingest user documents into a persistent (session) vector index. Use when the user uploads or provides links. Args: files_csv:str (comma-separated file paths), urls_csv:str (comma-separated URLs), chunk_size:int=1200, chunk_overlap:int=200', args_schema=<class 'langchain_core.utils.pydantic.doc_index'>, func=<function doc_index at 0x10b28d4e0>),\n",
       " StructuredTool(name='doc_rag_search', description=\"Query the user's indexed documents (from doc_index). Use for questions that refer to the uploaded/added documents. Args: query:str, k:int=4\", args_schema=<class 'langchain_core.utils.pydantic.doc_rag_search'>, func=<function doc_rag_search at 0x10b28d760>)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81c15a-779e-4962-91b0-62b91b66264f",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "427ad83f-213e-4ec6-bd35-6d5d4afb4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d1c298cf-1ebd-4cc6-b782-4686445047a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nehagaonkar/Desktop/RAG agent/ragagent/lib/python3.12/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f37e9c87-9a6d-48f0-8d44-4b6f259946de",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=create_openai_tools_agent(llm,tools,prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a3065-dc7c-4210-87ab-07833468bc1e",
   "metadata": {},
   "source": [
    "## Agent Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ca30a008-f959-4cb5-ba9f-ac650d580377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x10593ea20>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x10593ea20>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x10ea977d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10ea97110>, root_client=<openai.OpenAI object at 0x10ea94710>, root_async_client=<openai.AsyncOpenAI object at 0x10ea96750>, model_name='gpt-3.5-turbo-0125', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'Wikipedia quick lookup. Use for short factual questions that need a brief summary. Do NOT use for detailed/ambiguous multi-part queries or when quotes/timelines are needed; prefer wikipedia_rag in those cases.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'wikipedia_rag', 'description': 'Deep Wikipedia RAG. Use when the question needs detailed context, comparisons, timelines, or quotations from Wikipedia beyond a short summary. Args: query:str, top_docs:int=3', 'parameters': {'properties': {'query': {'type': 'string'}, 'top_docs': {'default': 3, 'type': 'integer'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'ArXiv tool. Use when the query references arXiv or includes an arXiv ID.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith_search', 'description': 'FOR LANGSMITH ONLY. If the query mentions LangSmith or docs.smith.langchain.com, you MUST use this tool and no other.', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'web_rag_search', 'description': 'General web RAG. Use when the question is NOT about Wikipedia, Arxiv, or LangSmith. Args: query:str, count:int=5', 'parameters': {'properties': {'query': {'type': 'string'}, 'count': {'default': 5, 'type': 'integer'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'doc_index', 'description': 'Ingest user documents into a persistent (session) vector index. Use when the user uploads or provides links. Args: files_csv:str (comma-separated file paths), urls_csv:str (comma-separated URLs), chunk_size:int=1200, chunk_overlap:int=200', 'parameters': {'properties': {'files_csv': {'default': './NehaGaonkarDataScientist.pdf', 'type': 'string'}, 'urls_csv': {'default': '', 'type': 'string'}, 'chunk_size': {'default': 1200, 'type': 'integer'}, 'chunk_overlap': {'default': 200, 'type': 'integer'}}, 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'doc_rag_search', 'description': \"Query the user's indexed documents (from doc_index). Use for questions that refer to the uploaded/added documents. Args: query:str, k:int=4\", 'parameters': {'properties': {'query': {'type': 'string'}, 'k': {'default': 4, 'type': 'integer'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[WikipediaQueryRun(description='Wikipedia quick lookup. Use for short factual questions that need a brief summary. Do NOT use for detailed/ambiguous multi-part queries or when quotes/timelines are needed; prefer wikipedia_rag in those cases.', api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/Users/nehagaonkar/Desktop/RAG agent/ragagent/lib/python3.12/site-packages/wikipedia/__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=1000)), StructuredTool(name='wikipedia_rag', description='Deep Wikipedia RAG. Use when the question needs detailed context, comparisons, timelines, or quotations from Wikipedia beyond a short summary. Args: query:str, top_docs:int=3', args_schema=<class 'langchain_core.utils.pydantic.wikipedia_rag'>, func=<function wikipedia_rag at 0x118260900>), ArxivQueryRun(description='ArXiv tool. Use when the query references arXiv or includes an arXiv ID.', api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=3, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=4000)), Tool(name='langsmith_search', description='FOR LANGSMITH ONLY. If the query mentions LangSmith or docs.smith.langchain.com, you MUST use this tool and no other.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x108c7bd80>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x11855c440>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x1087dd4e0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x11855c440>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content')), StructuredTool(name='web_rag_search', description='General web RAG. Use when the question is NOT about Wikipedia, Arxiv, or LangSmith. Args: query:str, count:int=5', args_schema=<class 'langchain_core.utils.pydantic.web_rag_search'>, func=<function web_rag_search at 0x10969a200>), StructuredTool(name='doc_index', description='Ingest user documents into a persistent (session) vector index. Use when the user uploads or provides links. Args: files_csv:str (comma-separated file paths), urls_csv:str (comma-separated URLs), chunk_size:int=1200, chunk_overlap:int=200', args_schema=<class 'langchain_core.utils.pydantic.doc_index'>, func=<function doc_index at 0x10b28d4e0>), StructuredTool(name='doc_rag_search', description=\"Query the user's indexed documents (from doc_index). Use for questions that refer to the uploaded/added documents. Args: query:str, k:int=4\", args_schema=<class 'langchain_core.utils.pydantic.doc_rag_search'>, func=<function doc_rag_search at 0x10b28d760>)])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "710f845b-46ac-4a3d-b72c-2c63a0737cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pick_tools(q: str):\n",
    "#     ql = q.lower()\n",
    "#     if \"langsmith\" in ql or \"docs.smith.langchain.com\" in ql:\n",
    "#         return [retriever_tool]\n",
    "#     if \"arxiv\" in ql or re.search(r\"\\b\\d{4}\\.\\d{5}\\b\", ql):\n",
    "#         return [arxiv]\n",
    "#     # quick wiki signals\n",
    "#     if any(w in ql for w in [\"who is\", \"who was\", \"when was\", \"where is\", \"capital of\", \"definition of\"]):\n",
    "#         return [wiki, wikipedia_rag_tool]\n",
    "#     # default to web\n",
    "#     return tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8b294eb0-fd18-40e1-aeab-19e503148981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_rag_search` with `{'query': 'Experience of Neha'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mWeb RAG answer (supporting snippets):\n",
      "- Excerpt: Neha Kumar                                                          0                                     Investor, entrepreneur, business coach, faculty, founder, mom.    Meet Neha Kumar                           Neha Kumar brings over 15 years of experience in startups and a proven track record as a changemaker and strategic leader.Before co-founding Full Glass Wine, Neha served as COO and CFO of Create & Cultivate, where she successfully led the company through a private equity acquisition in 2021. Following the exit, she launched New Money Ventures, a fund to invest in early-stage startups ...\n",
      "  Source: https://www.nehatkumar.com/\n",
      "- Excerpt: At Full Glass Wine, Neha plays a pivotal role in identifying acquisition opportunities that align with the Full Glass mission, while also spearheading a rigorous vetting process to ensure each acquisition delivers long-term growth potential within the multi-brand strategy.Neha earned her undergraduate degree from UCLA and holds an MBA from USC.    F E A T U R E D  O N                                                                                                                L I S T E N     Jay Shetty sits down with Neha Kumar to talk about women in business and motherhood.          In a pla ...\n",
      "  Source: https://www.nehatkumar.com/\n",
      "- Excerpt: NEHA - National Environmental Health Association      Javascript is disabled on your browser. \t\tTo view this site, you must enable JavaScript or upgrade to a JavaScript-capable browser.\n",
      "  Source: https://www.neha.org/\n",
      "- Excerpt: C O N N E C T                “Neha is so inspiring! She is easy to talk to, and gave me some great ideas on how to unblock myself so I can move forward!”    ––  D A N I E L L E    “Neha is an excellent resource for guidance and support. She is very easy to communicate with and provided me with extremely helpful advice during our consultation.”    ––  J E N N I F E R    “Incredibly helpful. Feeling super motivated after our call.”    ––  A A S Y I A H                    Get in touch    For speaking engagements and press inquiries, please email pr@fullglass.wine\n",
      "  Source: https://www.nehatkumar.com/\n",
      "\n",
      "Top sources:\n",
      "* https://www.nehatkumar.com/\n",
      "* https://www.neha.org/\u001b[0m\u001b[32;1m\u001b[1;3mNeha Kumar has over 15 years of experience in startups and is known for her role as an investor, entrepreneur, business coach, faculty member, founder, and mother. She co-founded Full Glass Wine and previously served as the COO and CFO of Create & Cultivate, leading the company through a private equity acquisition in 2021. Neha also launched New Money Ventures, a fund investing in early-stage startups. She holds an undergraduate degree from UCLA and an MBA from USC.\n",
      "\n",
      "For more information, you can visit [Neha Kumar's website](https://www.nehatkumar.com/).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response1=agent_executor.invoke({\"input\":\"List experience of Neha\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f2fbd177-1a13-420a-b4e3-043b23d2427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': '1605.08386'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mPublished: 2016-05-26\n",
      "Title: Heat-bath random walks with Markov bases\n",
      "Authors: Caprice Stanley, Tobias Windisch\n",
      "Summary: Graphs on lattice points are studied whose edges come from a finite set of\n",
      "allowed moves of arbitrary length. We show that the diameter of these graphs on\n",
      "fibers of a fixed integer matrix can be bounded from above by a constant. We\n",
      "then study the mixing behaviour of heat-bath random walks on these graphs. We\n",
      "also state explicit conditions on the set of moves so that the heat-bath random\n",
      "walk, a generalization of the Glauber dynamics, is an expander in fixed\n",
      "dimension.\u001b[0m\u001b[32;1m\u001b[1;3mThe paper with the identifier 1605.08386 is titled \"Heat-bath random walks with Markov bases\" by Caprice Stanley and Tobias Windisch. The paper discusses the study of graphs on lattice points where the edges are derived from a finite set of allowed moves of arbitrary length. The authors demonstrate that the diameter of these graphs on fibers of a fixed integer matrix can be bounded by a constant. Additionally, the paper explores the mixing behavior of heat-bath random walks on these graphs and provides explicit conditions on the set of moves for the heat-bath random walk to act as an expander in a fixed dimension.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response2=agent_executor.invoke({\"input\":\"What's the paper 1605.08386 about?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e41e7b8d-954a-46e5-a746-d600bca6b45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'British king who established the Church of England'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Charles II of England\n",
      "Summary: Charles II (29 May 1630 – 6 February 1685) was King of Scotland from 1649 until 1651 and King of England, Scotland, and Ireland from the 1660 Restoration of the monarchy until his death in 1685.\n",
      "Charles II was the eldest surviving child of Charles I of England, Scotland and Ireland and Henrietta Maria of France. After Charles I's execution at Whitehall on 30 January 1649, at the climax of the English Civil War, the Parliament of Scotland proclaimed Charles II king on 5 February 1649. However, England entered the period known as the English Interregnum or the English Commonwealth with a republican government eventually led by Oliver Cromwell. Cromwell defeated Charles II at the Battle of Worcester on 3 September 1651, and Charles fled to mainland Europe. Cromwell became Lord Protector of England, Scotland and Ireland. Charles spent the next nine years in exile in France, the Dutch Republic and the Spanish Netherlands. A political crisis after Cromwel\u001b[0m\u001b[32;1m\u001b[1;3mThe British king who established the Church of England was King Henry VIII in the 16th century. He established the Church of England in 1534.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response3=agent_executor.invoke({\"input\":\"Who was the british king who established the church of England and in what year?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0a1a5685-5c04-4d27-be83-8e29a02e5578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Church of England establishment by British king and role of marriage'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Charles I of England\n",
      "Summary: Charles I (19 November 1600 – 30 January 1649) was King of England, Scotland, and Ireland from 27 March 1625 until his execution in 1649.\n",
      "Charles was born into the House of Stuart as the second son of King James VI of Scotland, but after his father inherited the English throne in 1603, he moved to England, where he spent much of the rest of his life. He became heir apparent to the kingdoms of England, Scotland, and Ireland in 1612 upon the death of his elder brother, Henry Frederick, Prince of Wales. An unsuccessful and unpopular attempt to marry him to Infanta Maria Anna of Spain culminated in an eight-month visit to Spain in 1623 that demonstrated the futility of the marriage negotiation. Two years later, shortly after his accession, he married Henrietta Maria of France.\n",
      "After his accession in 1625, Charles quarrelled with the English Parliament, which sought to curb his royal prerogative. He believed in the divine right of kings and was determined\u001b[0m\u001b[32;1m\u001b[1;3mThe British king who established the Church of England was King Henry VIII. His marriage played a significant role in this decision. Henry VIII's desire to divorce his first wife, Catherine of Aragon, led to a conflict with the Pope, ultimately resulting in the English Reformation and the establishment of the Church of England with the monarch as its head.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response4=agent_executor.invoke({\"input\":\"Who was the british king who established the church of England and what was the role of his marriage in his decisiopn?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c20a7e87-32b4-4731-aa11-795d8c6ff146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_rag_search` with `{'query': 'shooter of Charlie Kirk'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mWeb RAG answer (supporting snippets):\n",
      "- Excerpt: America.WorldFri, Sep 124:13'No idea the fire that you have ignited': Charlie Kirk's widow speaksThe conservative provocateur was speaking at an event on a college campus when he was killed.Crime and JusticeSaturday 2:35pm4:13Like the Robinsons, Schwiermann is a member of The Church of Jesus Christ of Latter-day Saints. She said they belonged to the same congregation, though she said the family hadn’t been active in the church in at least eight years.\"I want to make sure that people know that we don’t have any ill feelings towards their family or him,\" Schwiermann said. \"He made the wrong choi ...\n",
      "  Source: https://www.1news.co.nz/2025/09/14/students-reckon-with-trauma-after-witnessing-charlie-kirk-assassination/\n",
      "- Excerpt: Questions linger about alleged shooter’s motivation for killing Charlie Kirk | Charlie Kirk shooting | The Guardian\n",
      "  Source: https://www.theguardian.com/us-news/2025/sep/13/charlie-kirk-killing-alleged-shooter-motivation\n",
      "- Excerpt: at this timeSearchSearchFacebookTwitterEmailSMSPrintWhatsappRedditPocketFlipboardPinterestLinkedinLatest StoriesU.S. NewsPoliticsWorldSportsShoppingTiplineCultureBusinessHealthScienceCharlie Kirk assassination Suspect identifiedToxic rhetoricVance-Kirk tiesErika KirkBullet inscriptionsWhat we knowFirings over commentsGuns in AmericaSuspect in Charlie Kirk's killing had a new interest in politics and a 'squeaky clean' reputationUtah Gov. Spencer Cox said that a family member of the suspect's reached out to a family friend, who contacted the Washington County Sheriff's Office.Utah Gov. Spencer C ...\n",
      "  Source: https://www.nbcnews.com/news/us-news/suspect-charlie-kirk-shooting-tyler-robinson-rcna230504\n",
      "- Excerpt: What we know so far about the Charlie Kirk shooting suspect | PBS News                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...\n",
      "  Source: https://www.pbs.org/newshour/nation/what-we-know-so-far-about-the-charlie-kirk-shooting-suspect\n",
      "\n",
      "Top sources:\n",
      "* https://www.1news.co.nz/2025/09/14/students-reckon-with-trauma-after-witnessing-charlie-kirk-assassination/\n",
      "* https://www.theguardian.com/us-news/2025/sep/13/charlie-kirk-killing-alleged-shooter-motivation\n",
      "* https://www.nbcnews.com/news/us-news/suspect-charlie-kirk-shooting-tyler-robinson-rcna230504\n",
      "* https://www.pbs.org/newshour/nation/what-we-know-so-far-about-the-charlie-kirk-shooting-suspect\u001b[0m\u001b[32;1m\u001b[1;3mThe person identified as the shooter of Charlie Kirk is Tyler Robinson. If you would like more information, please let me know.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response5 = agent_executor.invoke({\"input\":\"Who is the person identified as the shooter of Charlie Kirk?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8a879d30-530f-42db-8158-c7d81c633a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_rag_search` with `{'query': 'Trump tariff on India'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mWeb RAG answer (supporting snippets):\n",
      "- Excerpt: SportHomeNewsSportBusinessInnovationCultureArtsTravelEarthAudioVideoLiveWeatherNewslettersTrump's 50% tariff on India kicks in as Modi urges self-reliance27 August 2025ShareSaveNikhil InamdarBBC News, MumbaiShareSaveBloomberg via Getty ImagesModi has urged small shop owners and businesses to put up \"Made in India\" boards outside their storesUS tariffs of 50% on goods from India took effect on Wednesday as Donald Trump sought to punish Delhi for buying Russian oil and weapons.The tariffs – among the highest in the world – include a 25% penalty for transactions with Russia that are a key source  ...\n",
      "  Source: https://www.bbc.com/news/articles/c5ykznn158qo\n",
      "- Excerpt: Donald Trump's 50% tariff on India kicks in as PM Modi urges self-relianceSkip to contentBritish Broadcasting CorporationHomeNewsSportBusinessInnovationCultureArtsTravelEarthAudioVideoLiveHomeNewsIsrael-Gaza WarWar in UkraineUS & CanadaUKUK PoliticsEnglandN. IrelandN. Ireland PoliticsScotlandScotland PoliticsWalesWales PoliticsAfricaAsiaChinaIndiaAustraliaEuropeLatin AmericaMiddle EastIn PicturesBBC InDepthBBC VerifySportBusinessExecutive LoungeTechnology of BusinessFuture of BusinessInnovationTechnologyScience & HealthArtificial IntelligenceAI v the MindCultureFilm & TVMusicArt & DesignStyleB ...\n",
      "  Source: https://www.bbc.com/news/articles/c5ykznn158qo\n",
      "- Excerpt: others to hold us in their clutches.\"He has since repeated these comments in at least two other public addresses this week. Modi's message to his countrymen has been loud and clear - both make in India and spend in India.The former has proved increasingly difficult, with the share of manufacturing as part of India's gross domestic product (GDP) stagnating at 15% levels, despite his government rolling out subsidies and production incentives over the years. But spurring long-pending tax reforms that immediately put more money into the hands of people could help the government soften some of the  ...\n",
      "  Source: https://www.bbc.com/news/articles/c5ykznn158qo\n",
      "- Excerpt: Trump presses European Union to impose 100% tariffs on India and China to pressure PutinSkip NavigationMarketsPre-MarketsU.S. MarketsEurope MarketsChina MarketsAsia MarketsWorld MarketsCurrenciesCryptocurrencyFutures & CommoditiesBondsFunds & ETFsBusinessEconomyFinanceHealth & ScienceMediaReal EstateEnergyClimateTransportationIndustrialsRetailWealthSportsLifeSmall BusinessInvestingPersonal FinanceFintechFinancial AdvisorsOptions ActionETF StreetBuffett ArchiveEarningsTrader TalkTechCybersecurityAIEnterpriseInternetMediaMobileSocial MediaCNBC Disruptor 50Tech GuidePoliticsWhite HousePolicyDefen ...\n",
      "  Source: https://www.cnbc.com/2025/09/10/trump-presses-european-union-to-impose-100percent-tariffs-on-india-and-china-to-squeeze-putin.html\n",
      "\n",
      "Top sources:\n",
      "* https://www.bbc.com/news/articles/c5ykznn158qo\n",
      "* https://www.cnbc.com/2025/09/10/trump-presses-european-union-to-impose-100percent-tariffs-on-india-and-china-to-squeeze-putin.html\u001b[0m\u001b[32;1m\u001b[1;3mDonald Trump imposed a 50% tariff on goods from India, which took effect as a measure to punish Delhi for buying Russian oil and weapons. The tariffs, considered among the highest in the world, include a 25% penalty for transactions with Russia, which is a key source. You can find more information on this topic in the following sources:\n",
      "1. [BBC News Article](https://www.bbc.com/news/articles/c5ykznn158qo)\n",
      "2. [CNBC Article](https://www.cnbc.com/2025/09/10/trump-presses-european-union-to-impose-100percent-tariffs-on-india-and-china-to-squeeze-putin.html)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response6 = agent_executor.invoke({\"input\":\"How much tariff has Trump levied on India?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5afee98d-8360-4956-ad51-d1b457781c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mSure, here's a programming joke for you:\n",
      "\n",
      "Why do programmers prefer dark mode?\n",
      "Because the light attracts bugs!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response7 = agent_executor.invoke({\"input\":\"Tell me a programming joke\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7c364a57-6a86-44d8-9e3e-c569c6cab075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_rag_search` with `{'query': 'latest updates from NASA on the possibility of life on Mars'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mWeb RAG answer (supporting snippets):\n",
      "- Excerpt: See all topics            Facebook      Tweet      Email        Link                 Link Copied!                                              Follow                                  Scientists believe intriguing leopard spots on a rock sampled by the Perseverance rover on Mars last year may have potentially been made by ancient life, NASA announced Wednesday. The team has also published a peer-reviewed paper in the journal Nature about the new analysis, though they say further study is needed.                   “After a year of review, they have come back and they said, listen, we can’t find  ...\n",
      "  Source: https://www.cnn.com/2025/09/10/science/nasa-mars-sapphire-falls-rock-sample\n",
      "- Excerpt: Nasa rover finds rocks on Mars with potential signs of past lifeSkip to contentBritish Broadcasting CorporationHomeNewsSportBusinessInnovationCultureArtsTravelEarthAudioVideoLiveHomeNewsIsrael-Gaza WarWar in UkraineUS & CanadaUKUK PoliticsEnglandN. IrelandN. Ireland PoliticsScotlandScotland PoliticsWalesWales PoliticsAfricaAsiaChinaIndiaAustraliaEuropeLatin AmericaMiddle EastIn PicturesBBC InDepthBBC VerifySportBusinessExecutive LoungeTechnology of BusinessFuture of BusinessInnovationTechnologyScience & HealthArtificial IntelligenceAI v the MindCultureFilm & TVMusicArt & DesignStyleBooksEntert ...\n",
      "  Source: https://www.bbc.com/news/articles/cd725pj0g9ro\n",
      "- Excerpt: SportHomeNewsSportBusinessInnovationCultureArtsTravelEarthAudioVideoLiveWeatherNewslettersLife on Mars? 'Leopard-spot' rocks could be biggest clue yet4 days agoShareSaveRebecca MorelleScience editorShareSaveNASA/JPLThe rocks are covered with unusual markings that look like leopard spotsUnusual rocks discovered on Mars contain the most tantalising evidence yet of potential past life on the Red Planet.The mudstones, found in a dusty riverbed by Nasa's Perseverance Rover, are dotted with intriguing markings nicknamed leopard spots and poppy seeds.Scientists believe these features contain minerals ...\n",
      "  Source: https://www.bbc.com/news/articles/cd725pj0g9ro\n",
      "- Excerpt: Martian rock contains ‘clearest sign’ of ancient life on Mars, NASA says | CNN                                                                             Ad Feedback           CNN values your feedback                                                             1. How relevant is this ad to you?                                                                                                                                2. Did you encounter any technical issues?                                                                                                                             Video pla ...\n",
      "  Source: https://www.cnn.com/2025/09/10/science/nasa-mars-sapphire-falls-rock-sample\n",
      "\n",
      "Top sources:\n",
      "* https://www.cnn.com/2025/09/10/science/nasa-mars-sapphire-falls-rock-sample\n",
      "* https://www.bbc.com/news/articles/cd725pj0g9ro\u001b[0m\u001b[32;1m\u001b[1;3mNASA has made significant discoveries related to the possibility of life on Mars. Some of the latest updates include the identification of intriguing leopard spots on a rock sampled by the Perseverance rover, which scientists believe may have potentially been made by ancient life. Further analysis and study are ongoing to explore this discovery. You can find more detailed information on these updates from the following sources:\n",
      "1. [CNN - NASA Mars Sapphire Falls Rock Sample](https://www.cnn.com/2025/09/10/science/nasa-mars-sapphire-falls-rock-sample)\n",
      "2. [BBC News - Life on Mars? 'Leopard-spot' rocks could be biggest clue yet](https://www.bbc.com/news/articles/cd725pj0g9ro)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response9 = agent_executor.invoke({\"input\":\"What are the latest updates from NASA on the possibility of life on Mars?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c05e9-61eb-4921-8dfe-7ae5ff830d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-agent",
   "language": "python",
   "name": "rag-agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
